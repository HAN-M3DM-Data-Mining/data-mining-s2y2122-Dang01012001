---
title: "Assigment - Naive Bayes Sabotaged"
author:
  - Kien Huynh Trung - Author
  - Dang Nguyen - Reviewer
date: March 24, 2022
output:
   html_notebook:
    toc: true
    toc_depth: 2
---

```{r}
library(tidyverse)
library(tm)
library(caret)
library(wordcloud)
library(e1071)
library(dplyr)
library(SnowballC)
```
---
NB-fakenews.csv

## Business Understanding
Identify fake news using Naive Bayes model. 

## Data Understanding
```{r}
url <- "https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/data-mining-s2y2122-kienhuynh2903/master/datasets/NB-fakenews.csv"
rawDF <- read.csv (url) #Dang: I would replace it with: read.csv("https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/data-mining-s2y2122-kienhuynh2903/master/datasets/NB-fakenews.csv")

rawDF<- rawDF[-c(1:15000),]
rawDF$id <- NULL 
rawDF$title <- NULL
rawDF$author <- NULL

rawDF <- rawDF %>% relocate(label, .before = text)
rawDF <- mutate(rawDF, label = recode(label,"0"= "ham", "1"= "spam"))
rawDF$label <- rawDF$label %>% factor %>% relevel("spam")
# Dang: Kien did not convert data into factors hence the codes in modeling don't work

spam <- rawDF %>% filter(label == "spam")
ham <- rawDF %>% filter(label == "ham")

wordcloud (spam$text, max.words = 20, scale = c(4, 0.8), colors= c("indianred1","indianred2","indianred3","indianred"))
wordcloud (ham$text, max.words = 20, scale = c(4, 0.8), colors= c("lightsteelblue1","lightsteelblue2","lightsteelblue3","lightsteelblue"))
gc()
```

## Data Preparation
```{r}
rawCorpus <- Corpus(VectorSource(rawDF$text))

cleanCorpus <- rawCorpus %>% tm_map(tolower) %>% tm_map(removeNumbers)

cleanCorpus <- cleanCorpus %>% tm_map(tolower) %>% tm_map(removeWords, stopwords()) %>% tm_map(removePunctuation)

cleanCorpus <- cleanCorpus %>% tm_map(stripWhitespace)

cleanCorpus <- cleanCorpus %>% tm_map(stemDocument)

cleanCorpus <- tm_map(cleanCorpus, removeWords, stopwords("english"))

tibble(Raw = rawCorpus$content[1:3], Clean = cleanCorpus$content[1:3])

cleanDTM <- cleanCorpus %>% DocumentTermMatrix
set.seed(1234)
trainIndex <- createDataPartition(rawDF$label, p = .75, 
                                  list = FALSE, 
                                  times = 1)
#Dang: Kien put p (data goes into training) 25%, which is not sufficient since he cut 15,000 data points already. therefore I increased p to 75% (around 4300 data points)

trainDF <- rawDF[trainIndex, ]
testDF <- rawDF[-trainIndex, ]

trainCorpus <- cleanCorpus[trainIndex]
testCorpus <- cleanCorpus[-trainIndex]

trainDTM <- cleanDTM[trainIndex, ]
testDTM <- cleanDTM[-trainIndex, ]

freqWords <- trainDTM %>% findFreqTerms(5000)

trainDTM <-  DocumentTermMatrix(trainCorpus, list(dictionary = freqWords))
testDTM <-  DocumentTermMatrix(testCorpus, list(dictionary = freqWords))

convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0) %>% factor(levels = c(0,1), labels = c("No", "Yes"))
}

nColsDTM <- dim(trainDTM)[2]
trainDTM <- apply(trainDTM, MARGIN = 2, convert_counts)
testDTM <- apply(testDTM, MARGIN = 2, convert_counts)
```
## Modeling
```{r}
nbayesModel <- naiveBayes(trainDTM, trainDF$label, laplace = 1)
predVec <- predict(nbayesModel, testDTM)
confusionMatrix(predVec, testDF$label, positive = "spam", dnn = c("Prediction", "True"))
```
## Evaluation and Deployment
The model gives the result of 77.6 percent of accuracy level. I tried to remove unnecessary words but it seems not to be helpful. 

Hence, the reviewer may add suggestion to improve the model. 
# Dang: I had an idea of deleting all the unneccesary words by apply gsub and str_remove_all codes in the rawDF phase. However, it did not seem to work

